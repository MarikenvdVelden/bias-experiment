---
output: 
  stevetemplates::article:
    fig_caption: true
    extra_dependencies: ["float"]
bibliography: references.bib
urlcolor: black
linkcolor: black
header-includes:
   - \usepackage{floatrow}
   - \floatsetup[figure]{capposition=top}
   - \floatsetup[table]{capposition=top}
   - \usepackage{booktabs}
   - \usepackage{longtable}
   - \usepackage{array}
   - \usepackage{multirow}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage[para,online,flushleft]{threeparttable}
   - \usepackage{threeparttablex}
   - \usepackage[normalem]{ulem}
   - \usepackage{makecell}
   - \usepackage{xcolor}
   - \usepackage{hyperref}
   - \usepackage{array}   
   - \usepackage{caption}
   - \usepackage{graphicx}
   - \usepackage{epstopdf}
   - \usepackage{siunitx}
   - \usepackage{hhline}
   - \usepackage{calc}
   - \usepackage{tabularx}
   - \usepackage{fontawesome}
   - \usepackage{amsthm}
   - \newtheorem{hypo}{Hypothesis}
   - \usepackage{setspace}
biblio-style: apsr
title: "Whose Truth is it Anyway? An Experiment on Annotation Bias in Times of Factual Opinion Polarization"
author:
- name: Mariken A.C.G. van der Velden
- affiliation: Dep. of Communciation Science, Vrije Universiteit Amsterdam
- name: Myrthe Reuver
- affiliation: Computational Linguistics & Text Mining Lab, Vrije Universiteit Amsterdam
- name: Wouter van Atteveldt
- affiliation: Dep. of Communciation Science, Vrije Universiteit Amsterdam
- name: Antse Fokkens
- affiliation: Computational Linguistics & Text Mining Lab, Vrije Universiteit Amsterdam
- name: Felicia Loecherbach
- affiliation: Center for Social Media & Politics, New York University
- name: Kasper Welbers
- affiliation: Dep. of Communciation Science, Vrije Universiteit Amsterdam
thanks: "Corresponding author: MACGvdV, Replication files are available on the author's Github account (https://github.com/MarikenvdVelden/bias-experiment); Author contributions: a) designed the study: MACGvdV, MR, WvA, AF, FL, & KW; b) conducted the study: MACGvdV, FL & MR; c) data cleaning & analysis: MACGvdV; d) writing of the paper: MACGvdV, MR, WvA, AF, FL, & KW"
anonymous: TRUE
abstract: "The behavior of citizens and the study of them by social scientists rely heavily on key information. Human annotation represents a crucial instrument in the toolkit of the social sciences. To address concerns regarding the validity and reliability of annotation tasks, we establish strict standards. Due to the democratization of data and the advances in NLP more data can be analyzed or classified, making these standards are more important than ever: An algorithm trained on biased data will reproduce and often exacerbate bias. Our tools to create valid and reliable annotation data does currently not allow for dealing with different perspectives of annotators. In two pre-registered experiments in the United States and the Netherlands, we show that personal characteristics of annotators, like political ideology or knowledge, interfere with annotators' judgement of political stances. Our results show that to improve annotated data for automated text analyses, and for stance detection models in particular, we need to critically evaluate how we create our gold standards."
keywords: "Experiment, Annotation Bias, Ideology, Measurinig Political Position, Text-as-Data, Political Knowledge"
geometry: margin=1in
mainfont: cochineal
fontsize: 12pt
params:
  anonymous: ""
endnote: no
pandocparas: TRUE
sansitup: FALSE
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message=FALSE, warning=FALSE,
                      fig.pos = "h", out.extra = "")
options(kableExtra.latex.load_packages = FALSE)
options(scipen = 1, digits = 2)
source(here::here("src/lib/functions.R"))
# Uncomment below if you want code captions
# oldSource <- knitr::knit_hooks$get("source")
# knitr::knit_hooks$set(source = function(x, options) {
#   x <- oldSource(x, options)
#   x <- ifelse(!is.null(options$code.cap), paste0(x, "\\captionof{chunk}{", options$code.cap,"}"), x)
#   ifelse(!is.null(options$ref), paste0(x, "\\label{", options$ref,"}"), x)
# })
# Add `chunkcaption: TRUE` to YAML as well.
```

\doublespacing

# Introduction

The behavior of citizens and the study of them by social scientists rely heavily on key information.
<!--Word count: `r as.integer(sub("(\\d+).+$", "\\1", system(sprintf("wc -w %s", knitr::current_input()), intern = TRUE))) - 20`-->

# Introduction
The behavior of citizens and the study thereof by social scientists both rely heavily on key information.
Classical theories of political communication, such as agenda setting or framing [e.g. @vanaelst2016political; @lecheler2019news], formulate that political information drives opinion formation and participation in politics -- from voting to protests [for overviews hereof across countries, see @pfetsch2013comparing].
The democratization of data and advent of computational social science has paved the way for new possibilities of gathering and analyzing textual data [for a recent overview, see @vanatteveldt2022computational].
In particular, advances in Natural Language Processing (NLP) have made it possible to automatically analyze large quantities of data using machine learning [e.g., see @grimmer2022text; @akyurek-etal-2020-multi, @piskorski2023semeval].
To determine the validity of such large scale computational analyses, we rely on "gold standard" data, created by human annotators.
It follows that the validity of these analyses hinges on the quality of these gold standards.
If a gold standard contains biases, i.e., systematic errors, it can foster bias in any downstream analysis.

Most data collection efforts to create gold standards assume that there is only one correct interpretation for every input example, and that disagreement between the annotators is something that needs to be dealt with at all costs [@aroyo2015truth].
A recent study by @van2021validity demonstrates that using crowd-coding platforms is a good way to collect such gold standards without too much disagreement.
A recent study by @van2021validity demonstrates that using crowd-coding platforms is a good way to collect such golden standards without too much disagreement.
These platforms have also been used for experiments, and the quality of the respondents has been a center of attention [@coppock2019generalizing; @clifford2015samples; @huff2015these; @berinsky2014separating].
While it has been argued that this data is of similar quality to data from a random sample of the population [@coppock2019generalizing], others have demonstrated that these online platforms are populated by people that are "unlike" the general public, being younger and holding more liberal values [@clifford2015samples; @huff2015these].
If the latter is true, this might cause a problem for the coding of political texts: @ennser2018impact report that coders of political texts incorporate prior beliefs about parties' issue stances into their coding decisions.
The authors find that party labels cue coders to a stance.
For example, coders are more likely to report a left-wing party to be pro-immigration and a populist right-wing party to be against based on the exact same sentence.
*Is this actually bias or a diversity of view points? And how big of a problem does this bias/diversity of view points generate for researchers relying on golden standard data?* This question also taps into newer developments related to data annotations: Increasingly, Large Language Models such as ChatGPT are being used by researchers [@gilardi2023chatgpt] as well as annotators themselves [@veselovsky2023artificial] to "simulate" human respondents for annotation tasks.
These models draw among other things from the gold standard data sets that have been created by researchers, perpetuating the specific biases that are present in them and not allowing for any further disagreement or diversity of viewpoints.

There is a long history of text annotation in studies analyzing political text.
While this yields lots of experiences as to how to train coders so that we get reliable hand-coded data, the procedure is expensive, protracted, and sometimes does not even get us the quality of data we need [@weber18].
While the crowd offers a solution to some of these issues [@van2021validity], the main underlying assumption that there is one correct interpretation for every input example remains untouched.
However, especially for more complex coding tasks different research fields have different interpretations of what counts as *correct* solution depending on the main goal of annotation.
Do we want to create a clean benchmark data set that follows strict linguistic rules or one that reflects how statements are being interpreted by "real" readers that do not necessarily follow theoretical definitions on which elements are needed for a specified stance?
Trends of polarization have shown that people do interpret information according to their ideological position [e.g., @rekker2022understanding; @lee2021more].
*Does this mean that some are right and others are wrong? Or is there an ideological difference in the ground truth?* These questions present a fundamental challenge to the main way of working when collecting gold standard data, as we operate from the baseline assumption that disagreement among the annotators should be avoided or reduced.
Typically, when specific cases continuously cause disagreement, more instructions are added to limit interpretations (for recent innovations to improve annotation, see for example: @barbera2021automated; @struthers2020bridging; @winter2020online; @debell2013harder; @benoit2016crowd, @ying2022topics).
However, work in computational linguistics has shown that increased annotation instructions do not increase quality [@parmar2023don].
This leaves us between a rock and a hard place.
Is there a potential bias in annotators that we should account for?

In this paper, we build upon the NLP literature on disagreement -- or bias -- in annotation [e.g., see @shen2021sounds; @geva2019we; @sommerauer2020penguin; @plank2014learning] and so-called perspectivism [@cabitza2023toward; @havens2022beyond] -- i.e. the adoption of methods that integrate the opinions and perspectives of the human subjects involved in the knowledge representation step of the machine learning processes [@cabitza2023toward].
This literature puts forward that disagreement can occur because of differences in ideological position or political knowledge [@shen2021sounds; @alkiek2022classification; @joseph2021mis].
This allows us to test the extent to which disagreement takes place, for what type of stances, as well as gives us directions on how to deal with the diversity in conceptions and the political heterogeneity that nowadays potentially occurs in our sample of annotators.
To do so, we have fielded two high-powered pre-registered experiments (see [here](link) and [here](link)) in the Netherlands -- a low-level polarized country -- and in the U.S. -- a high-level polarized country testing the effect of ideological distance between the annotator and the political actor in the text (H1), the effect of overinterpretation based on political knowledge or ideological engagement (H2), and an offered solution of masking the political actor to mitigate the effects of ideology and knowledge (H3).[^1]
In the experiments, we vary the level of specification with which a political actor takes a stance -- a declarative sentence versus a sentence where with some knowledge on politics, the stance might be inferred -- as well as whether the political actor is shown or masked with putting `[ACTOR]` instead of the political party.
We do this for four different political issues: *Environment*, *Immigration*, *Tax Policy*, and *EU* (for the Dutch case) or *Foreign Policy* (for the American case).
This country selection does not only allow us to showcase the scope conditions of disagreement due to different levels of political heterogeneity, but also differentiates between languages.
English is not only the most dominant language for computational text analysis in the social science [@baden2022three; @dolinsky2023threegaps], crowd-coders do not need to be first language speakers, given the dominance of English in our daily lives.
This is different for Dutch, it is a language spoken by a smaller community, typically first language speakers, yet still an often-enough researched case in computational text analysis in the social science [@baden2022three; @dolinsky2023threegaps].

[^1]: The data and research compendium is published on the [[main author's github page]](https://github.com/MarikenvdVelden/bias-experiment) -- anonymized for the review process.

Our results demonstrate that overall sentences where with some knowledge on politics the stance might be inferred are more likely to be overinterpreted by the crowd, inferring a position where none is explicitly given. 
This is problematic as these sentences are very common in political text -- like legislative debates or speeches -- as well as media reports.
Moreover, our results also demonstrate that for these disagreements in the crowd to occur, the level of polarization needs to be high.
We do find support for our hypotheses in the American context, but not in the Dutch context -- except for the situation where masking overcomes political knowledge.
In this case, we do find support in the Dutch experiment, but not in the American one.
Our findings thus underline the importance of taking disagreement seriously for the creation of gold standard text -- the bread-and-butter of all machine learning endeavours.
We should look beyond the majority vote and model it in the data, because if an algorithm is trained on biased data from disagreeing annotators, it will reproduce and often exacerbate that bias when it is applied to new data [e.g., see @prost2019debiasing].
To be able to model these characteristic of annotators, we should survey the characteristics of annotators when using the crowd (see Webb-Williams et al. 2023 for a similar argument, yet different annotator characteristics).

# Whose Truth is it Anyway? Disagreement & Perspectivism in Creating Gold Standard Data

Generating large data sets has become one of the main drivers of progress in natural language understanding.
In studies of political communication, the most familiar annotation tasks involve identifying theoretical concepts.
This includes noting the topic of the text, the position of the actor or the tone of the text.
Crowdcoding seems suitable for this task [@van2021validity].
Yet, having only a few workers annotate the majority of text of interest has raised concerns about data diversity and models' ability to generalize beyond the crowd-workers.
In a series of experiments, @geva2019we show that often models do not generalize well to annotations from annotators that did not contribute to the training set, suggesting that annotator bias should be monitored during data set creation.
One such potential bias, especially in times of increasing polarization [@iyengar2019origins; @gidron2019toward; @boxell2022cross], is based on ideological position of the annotator.
Given that annotators on crowd-coding platforms tend to be younger and hold more liberal values than the general public [@clifford2015samples; @huff2015these], this could potentially hamper the data diversity and generalizability of the model.
An additional reason to monitor the annotators' ideological position as a potential source of annotator bias is that a recent study in NLP showed that experiential factors influence the consistency of how political ideologies are perceived [@shen2021sounds].
Their finding challenges the "ground-truth" assumption we as researcher make that a position for example is either left-leaning or right-leaning.
People with different ideological backgrounds might experience that position differently.
This challenges our way of data collection.
We are interested in the effect of e.g. elite communication.
To study this, we allow for heterogeneous treatment effects in experimental work.
This indicates that we often do not assume that the treatment, often using text, has the same effect for different partisans.
Yet, at the same time, we forget or ignore that knowledge when creating large data-sets for our machine learning models.

The field of Natural Language Processing often works on automatically classifying texts on labels of concepts such as stance, sentiment, and political orientation.
These models are trained on data created by human annotators.
Often, this process has a final step where disagreements and differences in annotations are leveled by aggregating, averaging, or in other ways coming to a consensus on one label for one example, which is then seen as "ground truth" [@aroyo2015].
Differences from this ground truth label are seen as errors that need to be removed or sorted out.
Models then learn to predict labels for new examples based on this ground truth.
However, since several years there is some discussion on how realistic it is to have one label, especially for subjective or complex concepts and/or texts with multiple interpretations.
@aroyo2015 describes succinctly how the predominant annotation procedures for classification models run into myths such as "disagreement is bad" and "one annotation is enough".
@plank2014learning adds to this that underlying ambiguity and linguistic complexity should be considered for disagreement in annotations: not all linguistic examples are created equal.
Disagreement even occurs in seemingly objective tasks such as Part of Speech tagging [@fornaciari2021; @plank2014learning].

Another aspect to consider is that disagreement can be informative for the concept under measure -- sometimes agreement can be used to validate hypotheses about how universal the perceptions of such concepts are [@sommerauer2020would].
Additional doubts on smoothing out disagreement in annotation have focussed on the lack of diversity when only annotating with one label or annotator, leading to a homogeneity especially in subjective and social tasks [@geva2019we] such as hatespeech detection or political affiliation classification.
The question then is: *Whose perspective is being recorded in these datasets, and then later in the models trained on these datasets?* Framing arbitrary representations in data as "bias" misses the political character of data sets: There is no neutral data and no apolitical standpoint from where we can call out bias.
Data sets are always "a worldview" and, as such, data always remains biased" [@miceli2022studying p.5]. This is key to social scientist in general, and those studying political text in particular, since several tasks of interest are intrinsically societal, with answers that differ based on the make-up of the worldview of annotators. The answers of the annotators in turn influence how machine learning examples classify new models. For instance, hate speech and abuse detection are NLP tasks where race and gender of annotators influences both annotators and model performance [@gordon2022; @larimore2021reconsidering; @waseem2016]. Language is inherently connected to society and culture: @shenDarling analyze sentiment analysis, and find that human annotators lead to certain perspectives on sentiment being recorded; notably African American English dialects are often misunderstood by such models.

Most recently, new annotation paradigms have gone one step further by asking whether we are modelling the task, or the annotator [@geva2019we].
@pavlick2019 find that for the logical coherence task Natural Language Inference, annotators have several valid interpretations that are not reflected in one ground truth label.
They call for new training paradigms that can reflect "the full range of possible human inferences" [@pavlick2019, p.688].
Recent approaches in NLP have sought to explicitly incorporate disagreement and diversity in training data annotations.
@rottger2022two introduce the idea of an explicitly subjective annotation paradigm existing in addition to one focussed on one label and "ground truth".
Such a subjective annotation paradigm can be used for purposes where the goal is finding diverse perspectives on the task or concepts, and for models to model more accurately how humans interpret a task or text.
Additionally, "perspectivism" [@cabitza2023toward] is a paradigm and research agenda where different perspectives are explicitly incorporated in the training data, and used by models to provide more human-like classifications.
Another paradigm is "jury learning" [@gordon2022], in which machine learning models do not learn to replicate one specific ground truth, but are trained with different annotator juries to reflect the judgement of different populations.
In both approaches, demographic and other individual aspects of the annotator are explicitly mentioned and highlighted as having an influence on classification performance, but is used as an asset rather than as an error.

@troiano2021emotion [p.2] also note how for a complex concept such as the "emotion" of a text, the annotator can make several assumptions during the annotation process on what is wanted, that are all valid and may or may not be useful in different contexts: "It is possible to assess one's own emotion after reading the text, to reconstruct the affective state of the writers who produced it, to guess the reaction that they intended to elicit in the readers, and so on." @shen2021sounds find that political ideology is not an inherent concept in many texts, but rather dependent on who is asked to annotate and their perceptions and background.
Extralinguistic factors, such as annotators' own political ideology and also knowledge, influenced annotation and in turn model performance.
@thornjakobsen2022 specifically analyze a task related to stance detection, argumentative sentence detection, on these extralinguistic factors, and find an effect of gender and political leaning on annotations and also model performance.
However, to our knowledge this phenomenon has not yet been tested in a controlled experimental setting with manipulations in the texts.
To test whether people with different ideological backgrounds experience a political position differently, challenging our ground-truth assumption in data annotation, we propose the following hypothesis:

> **Ideological Bias hypothesis** (*H1a*): The larger the ideological distance between respondent and the party, the less likely respondents annotate statements according to the party's uttered position.

As noted by @plank2014, some linguistic examples are ambiguous, and open to multiple interpretations even for seemingly objective tasks such as part of speech tagging, where annotators have to distinguish parts of speech such as nouns and verbs.
A complex concept such as political ideology is much more likely to lead to multiple interpretations.
We call such sentences with more possible interpretations and less explicit standpoints "underspecified".
A lack of explicitness in the annotated text is one of the main causes of the disagreements in earlier literature.
@thornjakobsen2022 deduce that annotator bias comes from a process known as the affect heuristic [@slovic2007affect]: Making a decision based on the emotional response related to your own personal attitude towards the discussed topic, especially when the text is relatively ambiguous.
We therefore expect that the strength of H1a is conditional on this ambiguity:

> **Ideological Bias hypothesis** (*H1b*): The effect of H1a is stronger for underspecified sentences.

In the tradition of more strict interpretations of what constitutes as a stance especially in (computational) linguistics, sentences that are underspecified should always be annotated as "not a stance".
However, this might not be the desired annotation for other research purposes: When for example the main goal is to understand how readers are affected by statements shown in e.g., a newspaper article, a more lenient definition of stance that allows annotators to infer the direction of a political stance from context and prior knowledge even though a statement is strictly speaking underspecified might be more useful.
In everyday life, people will not follow strict linguistic definitions, for understanding media effects we might thus be more interested in whether stances, giving context information, are "correctly overinterpreted".
To test whether people with different ideological backgrounds as well as their political knowledge might experience underidentified position differently, challenging our ground-truth assumption in data annotation, we propose the following hypotheses:

> **Ideological Overinterpretation hypothesis** (*H2a*): The larger the ideological distance between respondent and the party, the more likely respondents interpret underspecified sentences as stance.

> **Political Knowledge Overinterpretation hypothesis** (*H2b*): The more political knowledge, the more likely people interpret underspecified sentences as stance.

In a next step, we ask: *If these biases exists, how can we alleviate them?* There have been several previous approaches to solve biased annotations, especially where it concerns political or societal aspects.
@geva2019we introduce an approach where training set annotators are separated from annotators annotating data sets that evaluate the models, to ensure the evaluation is not simply accurate at replicating the original annotators, but can generalize to new annotators' judgements.
However, these approaches are not aimed at reducing biases during the training set annotation procedure.
Other approaches are aimed at leveraging multiple perspectives - but this is not useful when one wants one label to learn from.
For the Austrian National Elections, @ennser2018impact already demonstrated that showing party labels impact annotators' assessment of the party position.
So, one solution is masking.
We therefore test whether masking reduces potential differentiation incited by ideological position or political knowledge with the following hypotheses:

> **Masking Solution hypothesis** (*H3a*): Masking reduces the effect of respondents' ideological position for coding stances according to the party's position.

> **Masking Solution hypothesis** (*H3b*): Masking reduces the effect of respondents' level of political knowledge for coding stances according to the party's position.

# Data, Methods & Measurement

## Data

We have conducted the survey experiments in the Netherlands in May 2022 and in the United States in January 2023.
Both samples, recruited through [KiesKompas](https://www.kieskompas.nl/en/) and [Prolific](https://www.prolific.co/) for respectively the Dutch and American case, consist of 3,000 participants (based on the power analysis presented in our [online compendium](LINK)) of 18 years and older.
Both survey companies works with non-random opt-in respondents.
Therefore, we measured many demographic background variables, and balance checks have been conducted to demonstrate whether certain categories are over represented in a certain experimental group.
Our study has been approved by the [Research Ethics Review Committee](https://fsw.vu.nl/nl/onderzoek/research-ethics-review/index.aspx) of the *Vrije Universiteit Amsterdam* (see the approval [here](https://github.com/MarikenvdVelden/bias-experiment/blob/master/docs/2022-3-30-59.pdf)).
To ensure good quality of our data, one attention check (discussed in more detail in **OA XX** is included [@berinsky2014separating].

## Measurement

***Experimental Conditions.*** Respondents are randomly assigned to either view a political party as an actor, or a masked condition, where they see `X` as an actor; simultaneously, respondents see either a fully specified sentence or a underspecified sentence, in which one needs additional information outside of the text to determine an actor's position.
Table \ref{tab:conditions} gives an overview of the variations in treatment in the surveys.

```{r conditions, topcaption=TRUE}
d <- tibble(`Condition` = c(rep("Specified", 4),
                          rep("Underspecified", 4)), 
            `US Experiment` = c("[Republicans/X] say immigration should be made more difficult.",
                              "[Democrats/X] say we need to put a tax on carbon emissions",
                              "[Democrats/X] say we should implement a wealth tax for the richest Americans.",
                              "[Republicans/X] say the U.S. needs to consider military build-up in the Pacific Ocean",
                              "[Republicans/ X] say many immigrants are crossing our borders.",
                              "[Democrats/X] say carbon emissions policy should be implemented differently.",
                              "[Democrats/X] say the tax system should be implemented differently.",
                              "[Republicans/ X] say there should be a different military presence in the Pacific Ocean."),
            `NL Experiment` = c("[PVV/X] says immigration should be made harder.",
                             "[GreenLeft/X] says nitrogen emissions need to be reduced.",
                             "[Labour Party/X] says tax rate should go up for highest earners.",
                             "[Forum for Democracy/X] says that membership in the European Union has been especially bad for the Netherlands so far.",
                             "[PVV/X] says many immigrants are coming this way.",
                             "[GreenLeft/X] says nitrogen policy must be different.",
                             "[Labour Party/X] says tax system must be changed.",
                             "[Forum for Democracy/X] says the Netherlands should have a different role in the European Union."))

kbl(d, booktabs =T, caption = "\\label{tab:conditions}Survey Questions - Experimental Conditions") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F, fixed_thead = T, position = "center") %>%
  column_spec(1, width = "3cm") %>%
  column_spec(2, width = "7cm") %>%
  column_spec(3, width = "7cm")
```

***Dependent Variable.*** We rely on whether or not a party's (implied) stance is coded according to the party's position (H1 and H3) as well as whether or not the statement is coded as a stance at all (H3).
For each issue, we ask the respondent `what is according to the sentence above the position of [ACTOR]?`, with the answer categories: `in favor`, `against`, `no stance`, `don't know`.
We use both a very strict interpretation of stance -- specification of change and direction -- and a lenient interpretation -- specification of change.
Using the strict interpretation, respondents are correct if they say `no stance` for the underspecified sentences and `against`, `in favor`, `in favor`, and `against` to the specified sentences one to four.
Using a more lenient interpretation, respondents could say either `in favor` or `against` as well for underspecified sentence two to four.

***Moderating Covariates.*** *Ideological position* is measured using an 11-point scale ranging from left (`0`) to right (`10`).
*Political knowledge* is measured with six items from the Dutch Parliamentary Election Studies for the Dutch sample, and the three items from the American National Election Studies.[\^]:The questionnaire can be found in OA **XX**.

***Control Variables.*** In our analysis, we control for demographic information (gender, age, education, income, religion, job) as well as political background variables (trust in politics, ideological position on economic left-right scale and cultural progressive-conservative scale, and evaluations and prospects of the economy).
Tables A.5 till A.17 in the OA demonstrate the descriptive information per country.

## Method

To test our hypotheses, we will conduct a multilevel model, with respondents clustered in issues, see Equation \ref{eq:pooled1}.
Using the pooled data we will estimate a within groups fixed effects model.
We have conducted a balance test based on demographics (age, gender, education, geographical region, level of urbanness, employment, and income), vote choice in the 2021 parliamentary elections, ideological self-placement, political knowledge, and positions on the issues, using the `cobalt` R package [@greifer2021].
This balance test indicated that none of the variables are unbalanced over the experimental groups, and therefore, as pre-registered, will not be added to the regression formula.
$Y\hat{Y}_{r, i, t}$ in Equation \ref{eq:pooled1} denotes the evaluation of a stance by respondent $r$, during issue $i$ and at experimental round $t$ -- ranging from round 1 to round 4.
The standard errors are clustered at the individual level.

```{=tex}
\begin{multline}\label{eq:pooled1}
    \hat{stance correct}_{r, i, t} =\beta_{0} + \beta_{1}masked_{r, i, t} +
    \beta_{2}specification_{r, i, t} + 
    \beta_{3}ideological distance to party_{r, i, t} +\\
    \beta_{4}political knowledge_{r, i, t} +
    \alpha_{i} + \gamma_{t} + \varepsilon_{r, i, t}
\end{multline}
```
# Results

To answer whether there is an ideological or knowledge-based annotation bias, we have conducted a two-by-two experiment.
Tables \ref{tab:profilenl} and \ref{tab:profileus} demonstrate the average profile of respondents who annotate correctly and incorrectly (where respondents who annotated some stances correctly and some incorrectly are weighted by proportion (in)correct).
In terms of demographics, there is not much of a difference.
Yet, people who are incorrectly identifying stances are more left-wing oriented compared to those who are correct -- i.e. an average score of 4 for those who are incorrect vs. an average score of 5 for those who are correct.
For other positions on issues or political knowledge, we do not see a difference in averages between those who are correctly and incorrectly identifying stances.
This profile is quite similar for the lenient interpretation of what a stance is.

```{r "profilenl-strict"}
load(here("data/intermediate/cleaned_data_nl.RData"))
load(here("data/intermediate/cleaned_data_us.RData"))
source(here("src/analysis/profile_resp.R"))
kbl(e1_nlp, booktabs =T, caption = "\\label{tab:profilenl}Profile Dutch Stance Annotators") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F, fixed_thead = T, position = "center") %>%
  column_spec(1, width = "7cm") %>%
  column_spec(2, width = "7cm")
```

```{r "profileus"}
kbl(e1_us_nlp, booktabs =T, caption = "\\label{tab:conditions_us}Profile American Stance Annotators") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F, fixed_thead = T, position = "center") %>%
  column_spec(1, width = "7cm") %>%
  column_spec(2, width = "7cm")
```

Looking at the effect of the experimental conditions on the four dependent variables -- 1) correctly identifying a stance; and 2) over-interpreting a stance for both a strict and a lenient interpretation of stance -- Figure \ref{fig:results-base} visualizes the baseline.
The left-hand panel demonstrates the effect of the two experimental conditions for correctly identifying the stance in the Dutch case.
The right-hand panel does so for the American case.
On average, many respondents in both cases (respectively `85%` in the Dutch case and `65%` in the American case) correctly interpreted the stance using either the lenient or strict interpretation (respectively in blue and red) -- as indicated by the intercept.
When we mask the political actor -- i.e. instead of mentioning the party, we put "X" -- we see that this does on average not improve correctly interpreting the stance significantly in neither the Dutch or the American case.
Additionally, we do see that the level of specification of a sentence has a significant effect.
If a sentence is not fully specified, it has a substantive negative effect on the likelihood to correctly interpret the stance in both the lenient and strict interpretation of a stance.
These effects are substantial in both the American and Dutch case, with coefficients varying between `-0.2` and `-0.5`.
This indicates that compared to a fully specified sentence, between `20%` and `50%` of the respondents are more likely to be incorrect when the sentence is under-specified -- that is when the sentence does not state a clear position, but mentions the issue.
Looking at the other dependent variable, whether they interpreted the sentence as a stance or not, we see that almost nobody overinterprets a stance in the strict interpretation in either the Dutch or American case.
Yet, they do overinterpret a stance in the lenient interpretation.
Moreover, if people see an X compared to a political actor, they are statistically significantly more likely to interpret the sentence as a stance in its strict interpretation.
Yet, a coefficient of `0.02` (i.e. `2%`) is a very small effect.
For the condition of specification level, however, we see that compared to a fully specified sentence, people seeing an under-specified sentence are much more likely to interpret the sentence as a stance in the strict interpretation: an increase of `0.83`.
This indicates that people do not excel in this task without any instruction.
Using the lenient interpretation, however, people seem less likely to annotate the sentence as a stance.
In the pre-registered section, we demonstrate the tests of the hypotheses, and afterwards, we discuss some explorations of the data to show the robustness of our findings, the visualizations thereof are displayed in OA **XX**.

```{r results-baseline, out.width = "95%", fig.align = 'center', fig.cap = "\\label{fig:results-base} Baseline Results of Experimental Conditions"}
knitr::include_graphics(here("report/figures", "baseline-1.png"))
```

## Pre-registered Results

First, we test whether there is an ideological bias in interpreting stances (H1a), and if this bias increases for those that are further away from the ideological position of the political actor in the under-specified condition (H1b).
Figure \ref{fig:results-h1} demonstrates on the left-hand panel the regression coefficients and on the right-hand panel the average marginal effects of the interaction between ideological distance and under-specified sentences.
The upper-left panel of Figure \ref{fig:results-h1} demonstrates the coefficient of ideological distances for the likelihood of interpreting the stance correctly.
There is a negligible positive effect -- a coefficient of `0.002` -- that is borderline significant.
Substantially, this means that there is no effect of ideological distance for correctly interpreting the stance in either the American or Dutch case.
Hence, no ideological bias found, thus no support for our H1a.
Looking at the lower-left, and right-hand panel of Figure \ref{fig:results-h1}, we see a small but significant effect of the interaction between ideological distance and the under-specified condition for both interpretations in the American case, and only for the strict interpretation of a stance in the Dutch case.
These effects are, however, going in the other direction than hypothesized.
Those who are ideologically further away from the party are less likely to be wrong than those who are close to the party, as shown in the right-hand panel of Figure \ref{fig:results-h1}.
For the Dutch case, using the strict interpretation, the difference is about `20%` -- from a coefficient of `-0.6` to `-0.4`.
In the American case, the slope is even steeper, with a difference of about `40%`.
We thus find evidence for H1b.
This demonstrates that even in times of heightened polarization, ideological annotation bias can be a concern.

Secondly, we hypothesized that there is a risk of over-interpretation from those that are ideologically distant to the political actor (H2a) as well as those who have high levels of political knowledge (H2b).
We measure this with an interaction between the experimental condition of specification and the variable of interest.
Following @brambor2006understanding, we visualize the average marginal effects for both interactions to enhance interpretation in the right-hand panel of Figure \ref{fig:results-h2}.
Figure \ref{fig:results-h2} demonstrates the average marginal effects for both interaction effects: In the upper-panel the results for H2a, and in the bottom-panel, the results for H2b.
In both countries, we see that the different interpretations of stance have opposite effects: A strict interpretation increases the chance of overinterpreting, but this is not exacerbated by ideological distance or political knowledge.
Using a more lenient interpretation, we see that this decreases the chance of overinterpreting.
While this is not further diminished by political knowledge, ideological distance does further diminish the change in the American case -- i.e. against expectation H2b.
In the Dutch case, however, the further you are from the party the more likely you are to overinterpret the sentence as a stance.
So, while we do find some support for our H2a, we will reflect on the interpretation of a stance in combination with the context, as this effects how crowds interpret underspecified sentences.

````{=tex}
\begin{landscape}
```{r results-h1, fig.align = 'center', fig.cap = "\\label{fig:results-h1} Ideological Distance", fig.pos="H", fig.show="hold",out.width = "49%", out.height="40%"}
knitr::include_graphics(c(here("report/figures", "h1-1.png"), here("report/figures", "h1-2.png")))
```
```{r results-h2, fig.align = 'center', fig.cap = "\\label{fig:results-h2} Results Level of Sentence Specification", fig.pos="H", fig.show="hold",out.width = "49%", out.height="40%"}
knitr::include_graphics(c(here("report/figures", "h2-1.png"), here("report/figures", "h2-2.png")))
```
\end{landscape}
````

Thirdly, we test whether masking of the political actor is a solution for potentially misinterpreting the stance.
We hypothesized that masking should reduce the ideological bias (H3a) as well as the bias resulting from political knowledge (H3b).
We test these hypotheses with an interaction between the condition masking and the variables of interest and Figure \ref{fig:results-h3} demonstrates the regression coefficients.
The upper-left panel of Figure \ref{fig:results-h3} shows, against expectation, a slight increase in the Dutch case: Those that are further away from the masked political actor are more likely to incorrectly interpret the stance.
The decrease in slope is however negligible: From approximately `0.1%` to `0.2%` of the respondents being incorrect.
There is no effect found in the American case.
The same goes for the interaction between masking and political knowledge, on the bottom-panel of Figure \ref{fig:results-h3}.
There is no significant effect found in the US, but a very small negative effect in the Dutch case.
This means that masking of political actors does not help to correctly interpret a sentence as a stance -- i.e. no support for H3a and H3b.

```{r results-h3, fig.align = 'center', fig.cap = "\\label{fig:results-h3} Results Masking Solution", fig.pos="H", fig.show="hold",out.width = "80%", out.height="80%"}
knitr::include_graphics(here("report/figures", "h3-1.png"))
```

## Exploratory Results

To check the robustness of our findings, Figure \ref{fig:results-exp1} demonstrates the analyses for each issue separately.
The different colors visualize the different dependent variables.
We do not see much variation between issues *Tax*, *EU/Foreign Policy*, and *Environment*.
For those issues, we see that almost everyone interprets the sentence correct (in blue and red).
We also see that for a lenient interpretation of stances, people are quite likely to overinterpret a position as a stance.
Being correct about the stance does not decrease when masking the political actor in both cases.
Yet, the chance of being correct decreases statistically significantly when the sentence is underspecified.
The same holds for overinterpreting for the lenient interpretation, but the opposite is true for the strict interpretation; there overinterpretation is more likely with underspecified sentences.
Looking at *Immigration*, we see a different pattern.
We see that masking does not increase the likelihood of being correct, but does increase the likelihood of overinterpretation regardless of how one defines a stance.
Underspecified sentences are less likely to be correctly identified and more likely to be overinterpreted regardless of the definition of a stance.
So, while there are some differences in effect sizes between the issues, the overall findings are not driven by a single issue.

In addition to issue-specific analyses, we also explore an interaction between treatments, visualized in Figure **OA.XX** for both dependent variables.
This shows that masking is of help when sentences are under-specified.
In the left-hand panel of Figure **OA.XX**, it demonstrates that for under-specified sentences, people are less likely to incorrectly identify a sentence as a stance when the actor is masked (coefficient of `-0.30`) than when an actor is revealed (coefficient of `-0.45`).
That means there is a 15% increase in having it correct.
The difference for overinterpreting is smaller between revealed and masked political actor -- shown in the right-hand panel of Figure **OA.XX** -- yet also statistically significant.
Compared to `85%` overinterpreting the sentence as a stance, in the masking solution "only" `80%` over-interprets the sentence as a stance.
In the recommendation section, we will reflect on the masking solution for under-specified sentences.

Lastly, we explore three different ways of measuring ideological distance and an alternative for political knowledge in the American case.
First, we measured ideological bias by looking at whether the respondent is congruent or not with the issue position in the sentence, visualized in Figure **OA.XX** .
Second, we measured ideological bias by looking at whether the person voted for the party displayed in the sentence, visualized in Figure **OA.XX**. And thirdly, we measured ideological bias by looking at the ideology of the respondents -- not in relation to the political actor revealed, visualized in Figure **OA.XX**. These figures show that our null-finding regarding ideological bias is not conditional upon the measure we used.
In none of the analyses, we find evidence for ideological bias.
Also for the alternative measurement of political knowledge in the US, we find the same results as reported in the main analyses.

```{r results-exploration1, out.height = "85%",out.width = "95%",, fig.align = 'center', fig.cap = "\\label{fig:results-exp1} Exploration: Issue Speficific Analyses", fig.pos="H", fig.show="hold"}
knitr::include_graphics(c(here("report/figures", "issues-1.png")))
```

# Discussion

Human annotation represents a crucial instrument in the toolkit of the social sciences.
In order to address concerns regarding the validity and reliability of annotation tasks, we establish strict standards.
Especially in times where due to the democratization of data and the advances in NLP more data can be analyzed or classified, these standards are more important than ever.
Human annotation is a crucial instrument in the toolkit of the social sciences.
In order to safeguard the validity and reliability of annotation tasks, strict standards need to be upheld.
Especially in these days, where due to the democratization of data and the advances in NLP more data can be analyzed or classified, these standards are more important than ever.
After all, an algorithm trained on biased data will reproduce and often exacerbate bias [e.g. see @prost2019debiasing].
It is key to realize that the underlying assumption to create valid and reliable annotated data is that there is only one correct interpretation for every input example, and that disagreement between the annotators is something that needs to be dealt with at all costs [@aroyo2015truth].
In times of increasing factual opinion polarization [@rekker2022understanding; @lee2021more], it is crucial to ask *whose perspective is being recorded in these datasets*, given that there is no neutral data and no apolitical standpoint from where we can call out bias: "Datasets are always 'a worldview' and, as such, data always remains biased" [@miceli2022studying, p.5].
Our tools to create valid and reliable annotation data does currently not allow for dealing with different perspectives.
In this paper, we examined annotation bias in the classification of political stances using two pre-registered crowd-coding experiments in the Netherlands and the United States.
We used both a strict and a lenient interpretation of stance to illustrate whether decisions made in the research process on what constitutes a stance influences the results.

First, we tested whether ideological bias affects the interpretation of a party's stance by analyzing whether a person's ideological distance to a party affects how they classify the party's stance.
Although we did not find a main effect of ideological distance (H1a), we did find evidence that the effect of ideological distance is greater for underspecified sentences (H1b) in the US, depending on how one operationalizes stances.
The exploratory analyses showed that these findings where robust against different specifications of ideological distance.
This conditional finding is important, because machine translation is on the rise [@licht2023cross; @de2018no]: Social scientists use this technique for example to comparatively analyze political stances.
Currently, most scholars using text analysis rely on English text or annotated data, translating the original source text to English [@baden2022three].
Our results show that the way a political stance is defined and the context, potentially the level of polarization, affects whether annotators induced bias.
Hence, naively using US annotated data and auto-translating your source text could have downstream consequences for the classification of your stances.

Second, we looked at differences in terms of overinterpretation, which we define as coding stance in an underspecified sentence (i.e. in which the stance is not explicitly made clear) using both a strict (computational linguistics) and more lenient interpretation of what a stance is.
In other words, we tested when annotators inferred the stance of the party on the issue based on prior knowledge or beliefs about the party.
We expected that overinterpretation is more likely if the ideological distance of the coder to the party is greater (H2a) and if the coder has more political knowledge (H2b).
Overall, our findings did not support these hypotheses.
With the more lenient interpretation of overinterpretation, we did find support for H2a for the Dutch context, but we found an opposite effect for the US.
While this does not support the hypothesis in terms of the direction of the bias, it does imply that ideological distance can affect how stance is processed.
Regarding political knowledge (H2b), our results showed again country differences as well as that the way one defines a stance matters.
Our findings, for H2b in particular, pose important implications.
For sentences with clear political stances on an issue (e.g., immigration should be made more difficult) annotators generally agreed on the stance that is expressed.
But if the sentence refers to the issue without making a clear stance (e.g., many immigrants are crossing our borders) we do see a clear effect of political knowledge.
This indicates that for annotation bias to occur, there has to be sufficient room for different interpretations.
If the stance is clearly specified, then there is more of a ground truth, and the opinions and perspectives of the annotator therefore matter less.
It is when the stance is underspecified that perspectivism [@cabitza2023toward; @havens2022beyond] comes into play.
When a party talks about a political issue without taking an explicit stance, annotators need to fill in the gaps and make their own inferences about the party's stance on this issue. Considering that in many political documents and specifically reporting on political news statements are not made explicit, underspecified sentences can be assumed to rather be the rule than the exception.
Our results therefore imply that political knowledge is indeed a relevant dimension in shaping how an annotator perceives political stances.

Finally, we investigated whether annotation biases due to ideological distance from the party (H3a) and political knowledge (H3b) could be alleviated by masking the political party.
Here we again found country differences as well as variation based on how a stance was defined.
We showed that masking has no effect to resolve bias in the US context, but it does help in a less polarized context like the Netherlands.
Given that the US is one of the front runners in terms of polarization, it is important to realize that this affects our annotators, and therefore annotated data, too.
Biases seems to be stronger and less easy to be resolved.

Our results imply that to improve annotated data for automated text analyses, and for stance detection models in particular, we need to critically evaluate how we create our gold standards.
In the best case scenario, ignoring personal differences on dimensions such as political ideology only adds noise to the data.
However, this is only the case if a sufficiently large sample of annotators is randomly drawn from the population.
In reality, gold standards are often the product of a handful of expert coders, that typically study or work at universities.
Even if a good amount of crowd coders is used, it cannot blindly be assumed that this provides a diverse and balanced reflection of political ideologies in society, as we also observed in our samples (Tables \ref{tab:profilenl} and \ref{tab:profileus}). Especially in times where an increasing amount of crowd coding answers are based on large language models such as ChatGPT which (by and large) simulate the same ideological position over and over again [@veselovsky2023artificial], reflecting on the composition and quality of annotators and how it influences gold standards becomes more than just a methodological exercise.
Our results indicate that an annotation team that is skewed towards one end of the political spectrum could indeed result in a biased gold standard.

Furthermore, if the perception of political stance is contingent on ideological differences, then it can be inherently problematic to conceptualize political stance, as expressed in a text, as a ground truth.
Depending on the goal of a study, it might be more appropriate to conceptualize political stance as having an inherently subjective quality, and strive to measure this subjective space.
As a general guideline, we propose making a distinction between content analysis research that makes inferences about the author of a text, and research that makes inferences about the audience.
For example, a study that compares political stances between news outlets to analyze political bias in the news does require a consistent and reliable measurement of political stances.
This could indeed require a ground-truth definition of political stance, that is drilled into annotators through well-defined and specific instructions and extensive training.
But if the study involves making inferences about how the audience interprets political stances, for example in media effects research, then a ground-truth definition of political stance is problematic.
To make accurate inferences about how a citizen subjectively perceives political content, and consequently how this might have affected behavior such as voting, we need to learn more about how shared personal characteristics relate to shared modes of perspective.

```{=tex}
\newpage
\singlespacing
```
# References
